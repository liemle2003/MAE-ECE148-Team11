# <div align="center">Need title</div>
![image](https://github.com/WinstonHChou/winter-2024-final-project-team-7/assets/68310078/0ba1c6cb-c9e0-4cf7-905a-f5f16e6bb2ca)
### <div align="center"> MAE 148 Final Project </div>
#### <div align="center"> Team 11 Fall 2024 </div>

<div align="center">
    <img src="images\ucsdyellow-car.jpg" height="300"> <img src="images\ucsdcart.png" height="300"><br>

</div>

## Table of Contents
  <ol>
    <li><a href="#team-members">Team Members</a></li>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#what-we-promised">What We Promised</a></li>
    <li><a href="#accomplishments">Accomplishments</a></li>
    <li><a href="#challenges">Challenges</a></li>
    <li><a href="#final-project-videos">Final Project Videos</a></li>
    <li><a href="#software">Software</a></li>
        <ul>
            <li><a href="#slam-simultaneous-localization-and-mapping">SLAM (Simultaneous Localization and Mapping)</a></li>
            <li><a href="#obstacle-avoidance">Obstacle Avoidance</a></li>
        </ul>
    <li><a href="#hardware">Hardware</a></li>
    <li><a href="#gantt-chart">Gantt Chart</a></li>
    <li><a href="#course-deliverables">Course Deliverables</a></li>
    <li><a href="#How To Run">Project Reproduction</a></li>
    <li><a href="#acknowledgements">Acknowledgements</a></li>
    <li><a href="#contacts">Contacts</a></li>
  </ol>

<hr>

## Team Members
- Liem Le - MAE-Ctrls & Robotics (MC34) - Class of 2025
- Jusung Park- MAE (MC81) - Class of 2025
- Anna Hsu - ECE (EC27) - Class of 2026

<hr>

## Abstract
* We propose developing an autonomous car navigation system using Rapidly-Exploring Random Trees (RRT) with LiDAR for real-time environment mapping and obstacle detection.  Using LiDAR, the system will create a virtual map, enabling the RRT algorithm to identify viable paths around obstacles. A local planner using the Regulated Pure Pursuit Controller will use generated waypoints to guide the car to the target, while real-time controls (x = u̇, v̇, θ̇) will manage the car’s movement. This setup allows for real-time decision-making, enabling the car to differentiate between obstacles on the fly and navigate dynamically toward its goal.

<hr>

## What We Promised
### Must Have
* Integrate 3D LiDAR with FAST LIO (LiDAR-inertial odometry package) to generate 3D pointcloud data and use point cloud data to get 2D laser scan data for SLAM
* Integrate cost map with RRT algorithm to show RRT pathing in real time and repathing when obstacles are detected within a certain radius of the robot
* Integrate Regulated Pure Pursuit Controller to follow waypoints generated by RRT algorithm in simulation

### Nice to Have
* Integrate 2D laser scan data onto costmap for obstacle mapping in real time
* Have map generation, RRT pathing, and controller

<hr>

## Accomplishments
* ODOM data achieved
  * Successfully implemented Fast LIO for real-time lidar odometry and mapping which enables the robot to accurately locate itself while generating a 3D map of the environment.
* Laserscan 2D visualization achieved
  * Configured visualization tools such as Rviz2 for odometry data and 3D point cloud representation, including tuning parameters for LaserScan and PointCloud2 to enhance data clarity.
* Map generation achieved
  * Successfully set up map generation with occupancy grid generated with SLAM to create set grid of specified size (default 10m x 10m) and show goal position and robot position in real time
* RRT path planning and repathing achieved
  * Successfully set up RRT algorithm to plan path around obstacles and replan when new obstacle is detected
* Regulated Pure Pursuit Controller implemented to follow waypoints
  * Successfully set up pure pursuit controller algorithm and utilized it to follow waypoints generated by RRT path planning script

<hr>

## Challenges
* Installing and setting up OMPL on ARM64 architecture (worked well on x86 architecture in simulation)
* Configuring SLAM to use 2D laser scan data
* Configuring VESC with our custom written controller code

<hr>

## Software

### Laserscan
- Laserscan was another key component to our project which helped enable efficient processing of lidar data for environmental mapping and localization within the ROS2 framework. By using LaserScan data, we were able to detect and visualize the robot’s surroundings in real time, providing a clear 2D representation of spatial layouts. This was also visualized using Rviz2 which optimized visualization settings for clarity.
## Fast_Lio and PointCloud
- The Fast LIO was integral to our project, which helped enable integration of lidar odometry and mapping into the ROS2 framework. This setup allowed the robot to process lidar data in real time and generate accurate odometry for localization  while simultaneously creating detailed point cloud representations of the environment. These point clouds were visualized using the tool, Rviz2 and this helped assess the environment in 3D which also helped the robot to map and detect obstacles.
### Odometry
- The odometry system was crucial for our project, which provided the foundation for accurate localization and navigation within the ROS2 framework. The system allowed the robot to determine its precise position and orientation relative to its environment. This was also visualized using Rviz2, which helped provide a clear and smooth tracking of the robot’s movement.

## Need to edit the gazebo/more softwares on

<br>
<hr>

## Hardware 

* __3D Printing:__ Camera Case & Stand, Jetson Nano Case, Jetson Nano Case, Base Mounts
* __Laser Cut:__ Base with 3mm holes to mount electronics and other components.

__Parts List__

* Traxxas Chassis with steering servo and sensored brushless DC motor
* Jetson Nano
* LIVOX MID 360
* Livox three-wire aviation connector
* Aviation connector power network port cable
* 64 GB Micro SD Card
* Adapter for micro SD card
* Wifi Antenna
* Logitech Controller (F710)
* OAK-D Lite Camera 
* SparkFun OpenLog Artemis (IMU)
* VESC
* XeRUn 3660 G2 Sensored Motor
* Anti-Spark Switch with Power Switch
* DC-DC Converter
* 3 cell LIPO Battery
* Battery Voltage Checker
* DC Barrel to XT30 Connector
* XT60, XT30, MR60 connectors

*For Testing:*

*Car Stand
*5V, 4A power supply for Jetson Nano
*USB-C to USB-A cable
*Micro USB to USB cable


### __Mechanical Designs__

__Base Plate with 3mm Holes__

<img src="images\baseplate.jpg" height="350">

__Adjustable Camera Stand__

<img src="images\cameram1.png" height="160"> <img src="images\cameram2.png" height="160"><br>

__GPS Mount/Stand__

<img src="images\gpsstand.jpg" height="300">

__Circuit Diagram__

<img src="images\circuitdiagram.png" height="300">

<hr>

## Gantt Chart
<div align="center">
    <img src="images\ganttchart.png" height="500">
</div>
<hr>

## How To Run



<hr>


## Acknowledgements
Special thanks to Professor Jack Silberman for delivering the course!  
Thanks to TA Winston Chou and Ta Alexander for giving suggestions to our project!  


**Programs Reference:**
* [UCSD Robocar Framework](https://gitlab.com/ucsd_robocar2)
* [Slam Toolbox](https://github.com/SteveMacenski/slam_toolbox.git)
* [Triton AI](https://github.com/Triton-AI)
* [FAST LIO](https://github.com/hku-mars/FAST_LIO)
* [OMPL](https://ompl.kavrakilab.org/)


README.md Format, reference to [spring-2024-final-project-team-7](https://github.com/UCSD-ECEMAE-148/winter-2024-final-project-team-7)

<hr>

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Contacts

* Liem Le - ltle@ucsd.edu
* Jusung Park - jup018@ucsd.edu
* Anna Hsu - a6hsu@ucsd.edu















ssh -X jetson@ucsd-yellow.local

Pulling docker images(files)
- `cd projects/ `
-  `ll `
-  `cd robocar// `
-  `docker ps -a `
-  `docker images `
-  `cat docker.sh `

To enter container:
- `docker start team11 `
-  `docker exec -it team11 bash `
To activate ros2:
-  `source /opt/ros/foxy/setup.bash `
-  `ros2 <command lines>  `

# All the code has to be in the ros2_ws directory.

Git.clone https://github.com/hku-mars/FAST_LIO/tree/ROS2
  - into src in the container (...) 
- `cd ~/path/to/your/container/workspace/src `
- `git clone <url to git hub> `
- `git clone -b ROS2 --single-branch https://github.com/hku-mars/FAST_LIO.git `

OR

vcs import < livox.repox & vcs import < racer.repos
- `cd ~/path/to/your/container/workspace/src `
- `vcs import < livox.repos`
- `vcs import < racer.repos`

# To get Nav2 stack:
Update package index
 `sudo apt update `

# Install nav2 packages
```
sudo apt install ros-<ros-distro>-navigation2 
ros-<ros-distro>-nav2-bringup
source /opt/ros/<ros-distro>/setup.bash
```

## Install livox_ros_driver2 and Livox-SDK/Livox-SDK2 (sent in discord)

Develop our own code for 
Check out robocar 

robocar/repos/racer.repos:

# Pointcloud->Laser scan
- https://github.com/ros-perception/pointcloud_to_laserscan/tree/foxy
#
In this code change the Triton AI to hku-mars/FAST LIO… 
`Git.clone https://github.com/hku-mars/FAST_LIO/tree/ROS2`
  - Change IP address of livox_ros_driver2/config/MID360_config.json
  - Change line 28 to 192.168.1.124
## Launching Lidar
`ros2 launch livox_ros_driver2 rviz_MID360_launch.py`

## Installing Livox-Pointcloud2
Launch `ros2 run livox_to_pointcloud2 livox_to_pointcloud2_node`

## Pointcloud-Laserscan


